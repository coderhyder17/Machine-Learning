{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline in Machine Learning is a way to simplify your workflow by combining multiple steps. It is very useful when you have to perform multiple transformations on your data before applying the final estimator. \n",
    "\n",
    "It helps in reducing the chances of data leakage and makes the code more readable and maintainable. In this notebook, we will see how to use the pipeline in machine learning using the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps involved in the pipeline in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The pipeline in machine learning involves the following steps:\n",
    "1. Data Preprocessing: This step involves handling missing values, encoding categorical variables, and scaling the features.\n",
    "2. Feature Selection: This step involves selecting the most important features from the dataset.\n",
    "3. Model Building: This step involves building a machine learning model using the selected features.\n",
    "4. Prediction: This step involves making predictions using the model.\n",
    "4. Model Evaluation: This step involves evaluating the performance of the model using different metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of using the pipeline in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of using the pipeline in machine learninng are:-\n",
    "1. Reduces the chances of data leakage: The pipeline ensures that the transformations are applied only to the training data and not to the test data, reducing the chances of data leakage.\n",
    "2. Makes the code more readable and maintainable: The pipeline makes the code more readable and maintainable by combining multiple steps into a single object.\n",
    "3. Automates the workflow: The pipeline automates the workflow by applying the transformations in a sequential manner.\n",
    "4. Improves the performance: The pipeline improves the performance of the model by ensuring that the transformations are applied consistently to the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In summary the pipeline in machine learning is a very useful tool that helps in simplifying the workflow by combining multiple steps. It reduces the chances of data leakage, makes the code more readable and maintainable, automates the workflow, and improves the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# selecting features and target variable\n",
    "X = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic['survived']\n",
    "\n",
    "# split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# column transformer for imputing missing values\n",
    "numeric_features = ['age', 'fare']\n",
    "category_features = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                      ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "processor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                            ('cat', categorical_transformer, category_features)])\n",
    "\n",
    "# create a pipeline with a random forest classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', processor), \n",
    "                           ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict the target variable\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning in pipeline is a way to optimize the hyperparameters of the model by using grid search or random search. It helps in finding the best hyperparameters for the model and improves the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212290502793296\n",
      "Best hyperparameters: {'model__max_depth': None, 'model__min_samples_split': 5, 'model__n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# selecting features and target variable\n",
    "X = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic['survived']\n",
    "\n",
    "# split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a pipeline with a random forest classifier\n",
    "pipeline = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                     ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "                     ('model', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# hyperparameters for tune\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],\n",
    "    'model__max_depth': [None, 5, 10],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Grid search cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# getting the best model\n",
    "pipeline = grid_search.best_estimator_\n",
    "\n",
    "# making the prediction on best model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print('Best hyperparameters:', grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
